{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poprawic wszytkie koncowki w plikach do zapisu w notebookach 5,6,7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modeling -  selection of the best tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Purpose of the action </b> - checking accuracy of prediction on test set using different types of tree-based models:\n",
    "- RandomForestClassifier\n",
    "- AdaBoostClassifier\n",
    "- XGBClassifier\n",
    "- CatBoostClassifier\n",
    "\n",
    "<b> </b>\n",
    "<b> Action plan </b>:\n",
    "- Test 20 diffrent models for each type\n",
    "- Use ParameterSampler to generate different models with random hyperparameters\n",
    "- Use training set for fitting model and use validation set for model evaluation \n",
    "- Select the best 5 models of each type and create one AveragingClassifier\n",
    "- Train the best base models(top 1) of each type model on all data (training and validation sets)\n",
    "- Do the same with AveragingClassifiers\n",
    "- Create one AveragingClassifier using the best one model(top 1) of each type\n",
    "- Create LargeAveragingClassifier from the previously created AveragingClassifier (each model contains the top 5 models of the same type)\n",
    "- Save odels for use in future\n",
    "- Compare prediction accuracy and other metrics on test set and save results for future purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Import nessesary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from modeling import Metrics, show_best_models, select_best_classifiers\n",
    "from classifiers import AveragingClassifier, LargeAveragingClassifier\n",
    "from preprocessing_pipelines import basic_preprocess_pipeline, ImportantFeaturesSelector\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Import data sets dedicated for tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sets for selecting best models of each type\n",
    "train_set = pd.read_csv(\"./preprocessed_data/processed_base_train_set.csv\", index_col=0)\n",
    "validation_set = pd.read_csv(\"./preprocessed_data/processed_base_validation_set.csv\", index_col=0)\n",
    "\n",
    "# data sets for final fiting and prediction\n",
    "train_set_all = pd.read_csv('./preprocessed_data/train_set_stage2.csv', index_col=0)\n",
    "test_set = pd.read_csv('./preprocessed_data/test_set_stage2.csv', index_col=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Split datasets to feature and label sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature and label sets for selecting models\n",
    "X_train, y_train = np.array(train_set.drop(columns='FTR')), np.array(train_set['FTR'])\n",
    "X_val, y_val = np.array(validation_set.drop(columns='FTR')), np.array(validation_set['FTR'])\n",
    "\n",
    "# feature and label sets for final training and prediction\n",
    "X_train_all, y_train_all = train_set_all.drop(columns='FTR'), np.array(train_set_all['FTR'])\n",
    "X_test, y_test = test_set.drop(columns='FTR'), np.array(test_set['FTR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Create placeholders to hold prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder to hold prediction results\n",
    "prediction_metrics = Metrics()\n",
    "\n",
    "# lists to hold model objects\n",
    "single_models = []\n",
    "averaging_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1  Select best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best 5 models from 20 tested models using multiprocessing and <b> ParameterSampler </b> for generating random parameters. Use accuracy_score on validation set as metric for models evaluation.\n",
    "Feature selection is made in the pipeline inside function for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place: 1\n",
      "RandomForestClassifier{'random_state': 5, 'n_estimators': 600, 'max_depth': 17}\n",
      "Accuracy score on validation set: 0.6909\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 2\n",
      "RandomForestClassifier{'random_state': 5, 'n_estimators': 1200, 'max_depth': 13}\n",
      "Accuracy score on validation set: 0.6909\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 3\n",
      "RandomForestClassifier{'random_state': 7, 'n_estimators': 1000, 'max_depth': 15}\n",
      "Accuracy score on validation set: 0.6909\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 4\n",
      "RandomForestClassifier{'random_state': 2, 'n_estimators': 800, 'max_depth': 11}\n",
      "Accuracy score on validation set: 0.6879\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 5\n",
      "RandomForestClassifier{'random_state': 4, 'n_estimators': 1400, 'max_depth': 19}\n",
      "Accuracy score on validation set: 0.6848\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define params for random grid search\n",
    "params_grid={\n",
    "   'n_estimators': [600, 800, 1000, 1200, 1400, 1600],\n",
    "   'max_depth': [7, 9, 11, 13, 15, 17, 19, 21],\n",
    "   'random_state': [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10] \n",
    "}\n",
    "    \n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # function selecting best classifiers using multiprocessing\n",
    "    best_models, best_scoring = select_best_classifiers(estimator=RandomForestClassifier, \n",
    "                                                        params_grid=params_grid,\n",
    "                                                        n_iter=20, \n",
    "                                                        random_state=23,\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train, \n",
    "                                                        X_val=X_val, \n",
    "                                                        y_val=y_val, \n",
    "                                                        verbose=1,\n",
    "                                                        n_best_models=5)\n",
    "    # show best selected models\n",
    "    show_best_models(best_models, best_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Extract single models from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1, clf_2, clf_3, clf_4, clf_5 = best_models[:,1][0].steps[1][1], best_models[:,1][1].steps[1][1], \\\n",
    "                                    best_models[:,1][2].steps[1][1], best_models[:,1][3].steps[1][1], \\\n",
    "                                    best_models[:,1][4].steps[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 Create compleated pipelines (with scaling, encoding and futures selection) for each individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all base preprocess pipeline and transformers come from module preprocessing_pipelines.py\n",
    "pipe_clf_1 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_1, 'basic') ),\n",
    "                        ('classification', clf_1)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_2 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_2, 'basic') ),\n",
    "                        ('classification', clf_2)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_3 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_3, 'basic') ),\n",
    "                        ('classification', clf_3)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_4 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_4, 'basic') ),\n",
    "                        ('classification', clf_4)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_5 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_5, 'basic') ),\n",
    "                        ('classification', clf_5)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4  Make AveragingClassifier from the best 5 selected models (pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_clf = AveragingClassifier(base_estimators=[pipe_clf_1,\n",
    "                                               pipe_clf_2,\n",
    "                                               pipe_clf_3,\n",
    "                                               pipe_clf_4,\n",
    "                                               pipe_clf_5],\n",
    "                              voting='soft')\n",
    "\n",
    "# print(avg_clf.base_estimators[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.5 Fit single and averaging models on the entire data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier AveragingRandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # train model on all data\n",
    "    pipe_clf_1.fit(X_train_all, y_train_all)\n",
    "    avg_clf.fit(X_train_all, y_train_all)\n",
    "\n",
    "    # give models a name\n",
    "    clf_1_name = f'{clf_1.__class__.__name__}'\n",
    "    avg_clf_name = f'Averaging{clf_1.__class__.__name__}'\n",
    "    print(clf_1_name, avg_clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.6 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction metrics for single classifier to placeholder\n",
    "prediction_metrics.add_metrics(pipe_clf_1, clf_1_name, X_test, y_test)\n",
    "\n",
    "# add prediction metrics for voting classifier to placeholder\n",
    "prediction_metrics.add_metrics(avg_clf, avg_clf_name, X_test, y_test)\n",
    "\n",
    "# add both classifiers to the lists (to create largest average classifiers)\n",
    "single_models.append( (pipe_clf_1) )\n",
    "averaging_models.append( (avg_clf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Select best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best 5 models from 20 tested models using multiprocessing and <b> ParameterSampler </b> for generating random parameters. Use accuracy_score on validation set as metric for models evaluation.\n",
    "Feature selection is made in the pipeline inside function for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place: 1\n",
      "AdaBoostClassifier{'random_state': 5, 'n_estimators': 70, 'learning_rate': 0.6, 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Accuracy score on validation set: 0.7152\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 2\n",
      "AdaBoostClassifier{'random_state': 5, 'n_estimators': 50, 'learning_rate': 0.8, 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Accuracy score on validation set: 0.7091\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 3\n",
      "AdaBoostClassifier{'random_state': 5, 'n_estimators': 20, 'learning_rate': 0.4, 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Accuracy score on validation set: 0.7\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 4\n",
      "AdaBoostClassifier{'random_state': 0, 'n_estimators': 20, 'learning_rate': 0.4, 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Accuracy score on validation set: 0.7\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 5\n",
      "AdaBoostClassifier{'random_state': 1, 'n_estimators': 90, 'learning_rate': 0.4, 'base_estimator': DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')}\n",
      "Accuracy score on validation set: 0.697\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define params for random grid search\n",
    "params_grid={\n",
    "   'base_estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2), \n",
    "                      DecisionTreeClassifier(max_depth=3), DecisionTreeClassifier(max_depth=4),\n",
    "                      DecisionTreeClassifier(max_depth=5)], \n",
    "   'n_estimators': [20, 30, 40, 50, 70, 80, 90, 100],\n",
    "   'learning_rate': [0.4, 0.6, 0.8, 1.0, 1.2, 1.4],\n",
    "   'random_state': [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10] \n",
    "}\n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # function selecting best classifiers using multiprocessing\n",
    "    best_models, best_scoring = select_best_classifiers(estimator=AdaBoostClassifier, \n",
    "                                                        params_grid=params_grid,\n",
    "                                                        n_iter=20, \n",
    "                                                        random_state=23,\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train, \n",
    "                                                        X_val=X_val, \n",
    "                                                        y_val=y_val, \n",
    "                                                        verbose=1,\n",
    "                                                        n_best_models=5)\n",
    "    # show best selected models\n",
    "    show_best_models(best_models, best_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 Extract single models from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1, clf_2, clf_3, clf_4, clf_5 = best_models[:,1][0].steps[1][1], best_models[:,1][1].steps[1][1], \\\n",
    "                                    best_models[:,1][2].steps[1][1], best_models[:,1][3].steps[1][1], \\\n",
    "                                    best_models[:,1][4].steps[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Create compleated pipelines (with scaling, encoding and futures selection) for each individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all base preprocess pipeline and transformers come from module preprocessing_pipelines.py\n",
    "pipe_clf_1 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_1, 'basic') ),\n",
    "                        ('classification', clf_1)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_2 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_2, 'basic') ),\n",
    "                        ('classification', clf_2)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_3 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_3, 'basic') ),\n",
    "                        ('classification', clf_3)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_4 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_4, 'basic') ),\n",
    "                        ('classification', clf_4)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_5 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_5, 'basic') ),\n",
    "                        ('classification', clf_5)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.4  Make AveragingClassifier from the best 5 selected models (pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_clf = AveragingClassifier(base_estimators=[pipe_clf_1,\n",
    "                                               pipe_clf_2,\n",
    "                                               pipe_clf_3,\n",
    "                                               pipe_clf_4,\n",
    "                                               pipe_clf_5],\n",
    "                              voting='soft')\n",
    "\n",
    "# print(avg_clf.base_estimators[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.5 Fit single and averaging models on the entire data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier AveragingAdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # train model on all data\n",
    "    pipe_clf_1.fit(X_train_all, y_train_all)\n",
    "    avg_clf.fit(X_train_all, y_train_all)\n",
    "\n",
    "    # give models a name\n",
    "    clf_1_name = f'{clf_1.__class__.__name__}'\n",
    "    avg_clf_name = f'Averaging{clf_1.__class__.__name__}'\n",
    "    print(clf_1_name, avg_clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.6 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction metrics for single classifier to placeholder\n",
    "prediction_metrics.add_metrics(pipe_clf_1, clf_1_name, X_test, y_test)\n",
    "\n",
    "# add prediction metrics for averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(avg_clf, avg_clf_name, X_test, y_test)\n",
    "\n",
    "# add both classifiers to the lists (to create largest average classifiers)\n",
    "single_models.append( (pipe_clf_1) )\n",
    "averaging_models.append( (avg_clf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 Select best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best 5 models from 20 tested models using multiprocessing and <b> ParameterSampler </b> for generating random parameters. Use accuracy_score on validation set as metric for models evaluation.\n",
    "Feature selection is made in the pipeline inside function for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place: 1\n",
      "XGBClassifier{'subsample': 0.7, 'scale_pos_weight': 0.8, 'reg_alpha': 0.0001, 'random_state': 1, 'n_estimators': 400, 'min_child_weight': 4, 'max_depth': 6, 'learning_rate': 0.02, 'gamma': 0.4, 'colsample_bytree': 0.9}\n",
      "Accuracy score on validation set: 0.7333\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 2\n",
      "XGBClassifier{'subsample': 0.9, 'scale_pos_weight': 1, 'reg_alpha': 1e-05, 'random_state': 8, 'n_estimators': 600, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.02, 'gamma': 0.6, 'colsample_bytree': 0.8}\n",
      "Accuracy score on validation set: 0.7333\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 3\n",
      "XGBClassifier{'subsample': 0.9, 'scale_pos_weight': 1, 'reg_alpha': 1e-06, 'random_state': 2, 'n_estimators': 700, 'min_child_weight': 2, 'max_depth': 4, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 0.7}\n",
      "Accuracy score on validation set: 0.7273\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 4\n",
      "XGBClassifier{'subsample': 0.9, 'scale_pos_weight': 1.2, 'reg_alpha': 0.0001, 'random_state': 3, 'n_estimators': 600, 'min_child_weight': 2, 'max_depth': 6, 'learning_rate': 0.005, 'gamma': 0.4, 'colsample_bytree': 0.9}\n",
      "Accuracy score on validation set: 0.7212\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 5\n",
      "XGBClassifier{'subsample': 0.7, 'scale_pos_weight': 1, 'reg_alpha': 0.0001, 'random_state': 5, 'n_estimators': 500, 'min_child_weight': 2, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 0.6, 'colsample_bytree': 0.9}\n",
      "Accuracy score on validation set: 0.7152\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define params for random grid search\n",
    "params_grid = {\n",
    "                  'random_state':[0, 1, 2 ,3 ,4, 5, 6, 7, 8, 9, 10],\n",
    "                  'n_estimators': [300, 400, 500, 600, 700], \n",
    "                  'learning_rate' : [0.005, 0.01, 0.02],\n",
    "                  'max_depth' : [3, 4, 5, 6, 7, 8],\n",
    "                  'min_child_weight': [2, 3, 4],\n",
    "                  'gamma':[0.2, 0.4, 0.6],\n",
    "                  'subsample' : [0.7, 0.8, 0.9],\n",
    "                  'colsample_bytree' : [0.7, 0.8, 0.9],\n",
    "                  'scale_pos_weight' : [0.8, 1, 1.2],\n",
    "                  'reg_alpha':[1e-4, 1e-5, 1e-6]\n",
    "              } \n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # function selecting best classifiers using multiprocessing\n",
    "    best_models, best_scoring = select_best_classifiers(estimator=XGBClassifier, \n",
    "                                                        params_grid=params_grid,\n",
    "                                                        n_iter=20, \n",
    "                                                        random_state=23,\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train, \n",
    "                                                        X_val=X_val, \n",
    "                                                        y_val=y_val, \n",
    "                                                        verbose=1,\n",
    "                                                        n_best_models=5)\n",
    "    # show best selected models\n",
    "    show_best_models(best_models, best_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 Extract single models from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1, clf_2, clf_3, clf_4, clf_5 = best_models[:,1][0].steps[1][1], best_models[:,1][1].steps[1][1], \\\n",
    "                                    best_models[:,1][2].steps[1][1], best_models[:,1][3].steps[1][1], \\\n",
    "                                    best_models[:,1][4].steps[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 Create compleated pipelines (with scaling, encoding and futures selection) for each individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all base preprocess pipeline and transformers come from module preprocessing_pipelines.py\n",
    "pipe_clf_1 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_1, 'basic') ),\n",
    "                        ('classification', clf_1)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_2 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_2, 'basic') ),\n",
    "                        ('classification', clf_2)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_3 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_3, 'basic') ),\n",
    "                        ('classification', clf_3)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_4 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_4, 'basic') ),\n",
    "                        ('classification', clf_4)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_5 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_5, 'basic') ),\n",
    "                        ('classification', clf_5)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4  Make AveragingClassifier from the best 5 selected models (pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_clf = AveragingClassifier(base_estimators=[pipe_clf_1,\n",
    "                                               pipe_clf_2,\n",
    "                                               pipe_clf_3,\n",
    "                                               pipe_clf_4,\n",
    "                                               pipe_clf_5],\n",
    "                              voting='soft')\n",
    "\n",
    "# print(avg_clf.base_estimators[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.5 Fit single and averaging models on the entire data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier AveragingXGBClassifier\n"
     ]
    }
   ],
   "source": [
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # train model on all data\n",
    "    pipe_clf_1.fit(X_train_all, y_train_all)\n",
    "    avg_clf.fit(X_train_all, y_train_all)\n",
    "\n",
    "    # give models a name\n",
    "    clf_1_name = f'{clf_1.__class__.__name__}'\n",
    "    avg_clf_name = f'Averaging{clf_1.__class__.__name__}'\n",
    "    print(clf_1_name, avg_clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.6 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction metrics for single classifier to placeholder\n",
    "prediction_metrics.add_metrics(pipe_clf_1, clf_1_name, X_test, y_test)\n",
    "\n",
    "# add prediction metrics for averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(avg_clf, avg_clf_name, X_test, y_test)\n",
    "\n",
    "# add both classifiers to the lists (to create largest average classifiers)\n",
    "single_models.append( (pipe_clf_1) )\n",
    "averaging_models.append( (avg_clf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.1 Select best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the best 5 models from 20 tested models using multiprocessing and <b> ParameterSampler </b> for generating random parameters. Use accuracy_score on validation set as metric for models evaluation.\n",
    "Feature selection is made in the pipeline inside function for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place: 1\n",
      "CatBoostClassifier{'verbose': 0, 'subsample': None, 'random_state': 6, 'n_estimators': 700, 'max_depth': 9}\n",
      "Accuracy score on validation set: 0.7485\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 2\n",
      "CatBoostClassifier{'verbose': 0, 'subsample': 0.9, 'random_state': 8, 'n_estimators': 600, 'max_depth': 7}\n",
      "Accuracy score on validation set: 0.7455\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 3\n",
      "CatBoostClassifier{'verbose': 0, 'subsample': 0.7, 'random_state': 2, 'n_estimators': 600, 'max_depth': 8}\n",
      "Accuracy score on validation set: 0.7455\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 4\n",
      "CatBoostClassifier{'verbose': 0, 'subsample': 0.7, 'random_state': 6, 'n_estimators': 500, 'max_depth': 10}\n",
      "Accuracy score on validation set: 0.7364\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Place: 5\n",
      "CatBoostClassifier{'verbose': 0, 'subsample': None, 'random_state': 9, 'n_estimators': 600, 'max_depth': 8}\n",
      "Accuracy score on validation set: 0.7364\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define params for random grid search\n",
    "params_grid = {\n",
    "                  'random_state':[0, 1, 2 ,3 ,4, 5, 6, 7, 8, 9, 10],\n",
    "                  'n_estimators': [None, 300, 400, 500, 600, 700], \n",
    "                  'max_depth' : [None, 4, 5, 6, 7, 8, 9, 10],\n",
    "                  'subsample' : [None, 0.6,0.7, 0.8, 0.9],\n",
    "                  'verbose': [0],\n",
    "              } \n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # function build a voting classifier using multiprocessing\n",
    "    best_models, best_scoring = select_best_classifiers(estimator=CatBoostClassifier, \n",
    "                                                        params_grid=params_grid,\n",
    "                                                        n_iter=20, \n",
    "                                                        random_state=23,\n",
    "                                                        X_train=X_train, \n",
    "                                                        y_train=y_train, \n",
    "                                                        X_val=X_val, \n",
    "                                                        y_val=y_val, \n",
    "                                                        verbose=1,\n",
    "                                                        n_best_models=5)\n",
    "    # show best selected models\n",
    "    show_best_models(best_models, best_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.2 Extract single models from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1, clf_2, clf_3, clf_4, clf_5 = best_models[:,1][0].steps[1][1], best_models[:,1][1].steps[1][1], \\\n",
    "                                    best_models[:,1][2].steps[1][1], best_models[:,1][3].steps[1][1], \\\n",
    "                                    best_models[:,1][4].steps[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.3 Create compleated pipelines (with sclaing, encoding and futures selection) for each individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all base preprocess pipeline and transformers come from module preprocessing_pipelines.py\n",
    "pipe_clf_1 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_1, 'basic') ),\n",
    "                        ('classification', clf_1)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_2 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_2, 'basic') ),\n",
    "                        ('classification', clf_2)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_3 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_3, 'basic') ),\n",
    "                        ('classification', clf_3)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_4 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_4, 'basic') ),\n",
    "                        ('classification', clf_4)\n",
    "                      ])\n",
    "\n",
    "pipe_clf_5 = Pipeline([ ('preprocess_pipeline', basic_preprocess_pipeline),\n",
    "                        ('feature_seletion', ImportantFeaturesSelector(clf_5, 'basic') ),\n",
    "                        ('classification', clf_5)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.4  Make AveragingClassifier from the best 5 selected models (pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_clf = AveragingClassifier(base_estimators=[pipe_clf_1,\n",
    "                                               pipe_clf_2,\n",
    "                                               pipe_clf_3,\n",
    "                                               pipe_clf_4,\n",
    "                                               pipe_clf_5],\n",
    "                              voting='soft')\n",
    "\n",
    "# print(avg_clf.base_estimators[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.5 Fit single and averaging models on the entire data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostClassifier AveragingCatBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # train model on all data\n",
    "    pipe_clf_1.fit(X_train_all, y_train_all)\n",
    "    avg_clf.fit(X_train_all, y_train_all)\n",
    "\n",
    "    # give models a name\n",
    "    clf_1_name = f'{clf_1.__class__.__name__}'\n",
    "    avg_clf_name = f'Averaging{clf_1.__class__.__name__}'\n",
    "    print(clf_1_name, avg_clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8.6 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction metrics for single classifier to placeholder\n",
    "prediction_metrics.add_metrics(pipe_clf_1, clf_1_name, X_test, y_test)\n",
    "\n",
    "# add prediction metrics for averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(avg_clf, avg_clf_name, X_test, y_test)\n",
    "\n",
    "# add both classifiers to the lists (to create largest average classifiers)\n",
    "single_models.append( (pipe_clf_1) )\n",
    "averaging_models.append( (avg_clf) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Merge single and averaging models in largest averaging models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.1 Create new largest averaging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeModelsAveragingClassifier LargeTreeModelsAveragingClassifier\n"
     ]
    }
   ],
   "source": [
    "# create models (all base model is already fitted)\n",
    "\n",
    "# as base models using single classifier\n",
    "average_tree_clf = AveragingClassifier(base_estimators=single_models, voting='soft')\n",
    "\n",
    "# as base models using averaging classifier\n",
    "large_average_tree_clf = LargeAveragingClassifier(base_estimators=averaging_models, voting='soft')\n",
    "\n",
    "# give models a name\n",
    "average_tree_clf_name = 'TreeModelsAveragingClassifier'\n",
    "large_average_tree_clf_name = 'LargeTreeModelsAveragingClassifier'\n",
    "print(average_tree_clf_name, large_average_tree_clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.2 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prediction metrics for averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(average_tree_clf, average_tree_clf_name, X_test, y_test)\n",
    "\n",
    "# add prediction metrics for large averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(large_average_tree_clf, large_average_tree_clf_name, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.9.3 Save models for future purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save single voting model using pickle library\n",
    "with open(f'./models/{average_tree_clf_name}.pickle', 'wb') as f:\n",
    "    # pickle the 'models'using the highest protocol available.\n",
    "    pickle.dump(average_tree_clf, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# save averaging voting model using pickle library\n",
    "with open(f'./models/{large_average_tree_clf_name}.pickle', 'wb') as f:\n",
    "    # pickle the 'models'using the highest protocol available.\n",
    "    pickle.dump(large_average_tree_clf, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Show all result in one table and save it for future purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.596273</td>\n",
       "      <td>0.703953</td>\n",
       "      <td>0.657895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AveragingRandomForestClassifier</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.573248</td>\n",
       "      <td>0.706312</td>\n",
       "      <td>0.647368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.747529</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AveragingAdaBoostClassifier</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.613497</td>\n",
       "      <td>0.733491</td>\n",
       "      <td>0.668421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.752695</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AveragingXGBClassifier</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.613497</td>\n",
       "      <td>0.743711</td>\n",
       "      <td>0.668421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.740004</td>\n",
       "      <td>0.673684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AveragingCatBoostClassifier</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.744272</td>\n",
       "      <td>0.663158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TreeModelsAveragingClassifier</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.738208</td>\n",
       "      <td>0.668421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LargeTreeModelsAveragingClassifier</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.673684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model  precision_score  recall_score  \\\n",
       "0              RandomForestClassifier         0.623377      0.571429   \n",
       "1     AveragingRandomForestClassifier         0.616438      0.535714   \n",
       "2                  AdaBoostClassifier         0.646341      0.630952   \n",
       "3         AveragingAdaBoostClassifier         0.632911      0.595238   \n",
       "4                       XGBClassifier         0.676471      0.547619   \n",
       "5              AveragingXGBClassifier         0.632911      0.595238   \n",
       "6                  CatBoostClassifier         0.637500      0.607143   \n",
       "7         AveragingCatBoostClassifier         0.628205      0.583333   \n",
       "8       TreeModelsAveragingClassifier         0.640000      0.571429   \n",
       "9  LargeTreeModelsAveragingClassifier         0.644737      0.583333   \n",
       "\n",
       "   f1_score  roc_auc_score  accuracy_score  \n",
       "0  0.596273       0.703953        0.657895  \n",
       "1  0.573248       0.706312        0.647368  \n",
       "2  0.638554       0.747529        0.684211  \n",
       "3  0.613497       0.733491        0.668421  \n",
       "4  0.605263       0.752695        0.684211  \n",
       "5  0.613497       0.743711        0.668421  \n",
       "6  0.621951       0.740004        0.673684  \n",
       "7  0.604938       0.744272        0.663158  \n",
       "8  0.603774       0.738208        0.668421  \n",
       "9  0.612500       0.735400        0.673684  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get prediction metric result lists from placeholder\n",
    "precision_score, recall_score, f1_score, roc_auc_score, accuracy_score = prediction_metrics.get_metrics()\n",
    "\n",
    "# get model names list from placeholder\n",
    "models_name = prediction_metrics.get_names()\n",
    "\n",
    "# create dictionary of results \n",
    "results_dict = {'precision_score': precision_score, \n",
    "               'recall_score': recall_score, \n",
    "               'f1_score': f1_score,\n",
    "               'roc_auc_score' : roc_auc_score,\n",
    "               'accuracy_score' : accuracy_score}\n",
    "\n",
    "results_df = pd.DataFrame(data=results_dict)\n",
    "results_df.insert(loc=0, column='Model', value=models_name)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"./results/tree_models_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
