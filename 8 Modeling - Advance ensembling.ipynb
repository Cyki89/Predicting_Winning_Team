{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Modeling - Advance ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Purpose of the action </b> - create 4 advance machine learning models:\n",
    "- AveragingClassifier consisting of best single models of each type from previous parts\n",
    "- LargeAveragingClassifier consisting averaging models of each type from previous parts\n",
    "- StackClassifier classifier consisting of:\n",
    "    - base models - best single models of each type from previous parts\n",
    "    - meta model - will be determined based on the evaluation of different models\n",
    "- LargeStackClassifier classifier consisting of:\n",
    "    - base models - averaging models of each type from previous parts\n",
    "    - meta model - will be determined based on the evaluation of different models\n",
    "\n",
    "![title](Stack_Classifier.png)\n",
    "<i> Schematic of a stacking classifier framework. Here, three classifiers are used in the stack and are individually trained. Then, their predictions get stacked and are used to train the meta-classifier. </i> \n",
    "\n",
    "<b>Source</b> https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840\n",
    " \n",
    "<b> </b>\n",
    "<b> Action plan </b>:\n",
    "- Loads AveragingClassifiers and LargeAveragingClassifier from previous parts\n",
    "- Extract single models from AveragingClassifiers for future use as base learners in StackingClassifiers\n",
    "- Create AveragingClassifier consisting of best single models of each type (all base models is already fitted)\n",
    "- Create LargeAveragingClassifier consisting of averaging models of each type (all base models is already fitted)\n",
    "- Create a TestStackClassifier consisting of the best individual models of each type and several meta models to choose the best (evaluate on validation set)\n",
    "- Train StackClassifier with the best meta model on all data (training and validation sets)\n",
    "- Create a TestLargeStackClassifier consisting of five best models of each type and several meta models to choose the best (evaluate on validation set)\n",
    "- Train LargeStackClassifier with the best meta model on all data (training and validation sets)\n",
    "- Compare prediction accuracy and other metrics on the test set and save results for future purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Import nessesary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from modeling import Metrics\n",
    "from preprocessing_pipelines import basic_preprocess_pipeline, categorical_preprocess_pipeline, ImportantFeaturesSelector\n",
    "from classifiers import AveragingClassifier, LargeAveragingClassifier, StackClassifier, FastStackClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Import raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for training final models and prediction\n",
    "X_train = pd.read_csv('./preprocessed_data/train_set_stage2.csv', index_col=0)\n",
    "X_test = pd.read_csv('./preprocessed_data/test_set_stage2.csv', index_col=0)\n",
    "y_train = np.array(X_train['FTR'])\n",
    "y_test = np.array(X_test['FTR'])\n",
    "\n",
    "# data for evaluate meta models of StackClassifiers\n",
    "# split train set to training set and validation set\n",
    "break_point = X_train.shape[0]//19 # 19 - total number of seasons in train set\n",
    "X_train_eval = X_train.iloc[:-break_point]\n",
    "X_val_eval = X_train.iloc[-break_point:] # last season from training set\n",
    "y_train_eval = np.array(X_train_eval['FTR'])\n",
    "y_val_eval = np.array(X_val_eval['FTR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Import all previously prepared AveragingClassifiers and LargeAveragingClassifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/LinearModelsAveragingClassifier.pickle', 'rb') as f:\n",
    "    linear_averaging_clf = pickle.load(f)\n",
    "    \n",
    "with open('./models/LargeLinearModelsAveragingClassifier.pickle', 'rb') as f:\n",
    "    large_linear_averaging_clf = pickle.load(f) \n",
    "    \n",
    "with open('./models/TreeModelsAveragingClassifier.pickle', 'rb') as f:\n",
    "    tree_averaging_clf = pickle.load(f) \n",
    "\n",
    "with open('./models/LargeTreeModelsAveragingClassifier.pickle', 'rb') as f:\n",
    "    large_tree_averaging_clf = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Extract single and averaging models from AveragingClassifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all models are extracted along with their pipes with scaling, encoding and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract best tree-based estimators\n",
    "clf_rf, clf_ada, clf_xgb, clf_cat = tree_averaging_clf.base_estimators\n",
    "\n",
    "# extract best linear estimators\n",
    "clf_lr, clf_svc, clf_rbf, clf_knn = linear_averaging_clf.base_estimators\n",
    "\n",
    "# extract average tree-based estimators\n",
    "avg_clf_rf, avg_clf_ada, avg_clf_xgb, avg_clf_cat = large_tree_averaging_clf.base_estimators\n",
    "\n",
    "# extract average linear estimators\n",
    "avg_clf_lr, avg_clf_svc, avg_clf_rbf, avg_clf_knn = large_linear_averaging_clf.base_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Create placeholder to hold prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder to hold prediction results\n",
    "prediction_metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Create AveragingAllModelsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base models are the best single classifier of each type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.1 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all base models is already trained\n",
    "average_all_models_clf = AveragingClassifier(base_estimators=[*linear_averaging_clf.base_estimators,\n",
    "                                                              *tree_averaging_clf.base_estimators], \n",
    "                                             voting='soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.2 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give model a name\n",
    "average_all_models_clf_name = 'AveragingAllModelsClassifier'\n",
    "\n",
    "\n",
    "# add prediction metrics for large averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(average_all_models_clf, average_all_models_clf_name, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Create LargeAveragingAllModelsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base models are AveragingClassifiers of each type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1 Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all base models is already trained\n",
    "large_average_all_models_clf = LargeAveragingClassifier(base_estimators=[*large_linear_averaging_clf.base_estimators,\n",
    "                                                                         *large_tree_averaging_clf.base_estimators], \n",
    "                                                        voting='soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.2 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give model a name\n",
    "large_average_all_models_clf_name = 'LargeAveragingAllModelsClassifier'\n",
    "\n",
    "\n",
    "# add prediction metrics for large averaging classifier to placeholder\n",
    "prediction_metrics.add_metrics(large_average_all_models_clf, large_average_all_models_clf_name, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Create list of meta models for StackingClassifiers to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_models_to_check = [\n",
    "    LogisticRegression(penalty='l2', C=0.01, solver='lbfgs', max_iter=1000),\n",
    "    LogisticRegression(penalty='l2', C=0.1, solver='lbfgs', max_iter=1000),\n",
    "    LogisticRegression(penalty='l2', C=1, solver='lbfgs', max_iter=1000),\n",
    "    LogisticRegression(penalty='l2', C=10, solver='lbfgs', max_iter=1000),\n",
    "    LogisticRegression(penalty='l2', C=100, solver='lbfgs', max_iter=1000),\n",
    "    LogisticRegression(penalty='l1', C=0.01, solver='liblinear', max_iter=1000),\n",
    "    LogisticRegression(penalty='l1', C=0.1, solver='liblinear', max_iter=1000),\n",
    "    LogisticRegression(penalty='l1', C=1, solver='liblinear', max_iter=1000),\n",
    "    LogisticRegression(penalty='l1', C=10, solver='liblinear', max_iter=1000),\n",
    "    LogisticRegression(penalty='l1', C=100, solver='liblinear', max_iter=1000),\n",
    "    SVC(kernel='linear', C=0.001, probability=True),\n",
    "    SVC(kernel='linear', C=0.01, probability=True),\n",
    "    SVC(kernel='linear', C=0.1, probability=True),\n",
    "    SVC(kernel='linear', C=1, probability=True),\n",
    "    SVC(kernel='linear', C=10, probability=True),\n",
    "    SVC(kernel='linear', C=100, probability=True),\n",
    "    SVC(kernel='linear', C=1000, probability=True),\n",
    "    RandomForestClassifier(criterion='gini', n_estimators=10),\n",
    "    RandomForestClassifier(criterion='gini', n_estimators=100),\n",
    "    RandomForestClassifier(criterion='gini', n_estimators=500),\n",
    "    RandomForestClassifier(criterion='gini', n_estimators=1000),\n",
    "    RandomForestClassifier(criterion='entropy', n_estimators=10),\n",
    "    RandomForestClassifier(criterion='entropy', n_estimators=100),\n",
    "    RandomForestClassifier(criterion='entropy', n_estimators=500),\n",
    "    RandomForestClassifier(criterion='entropy', n_estimators=1000),\n",
    "    ExtraTreesClassifier(criterion='gini', n_estimators=10),\n",
    "    ExtraTreesClassifier(criterion='gini', n_estimators=100),\n",
    "    ExtraTreesClassifier(criterion='gini', n_estimators=500),\n",
    "    ExtraTreesClassifier(criterion='gini', n_estimators=1000),\n",
    "    ExtraTreesClassifier(criterion='entropy', n_estimators=10),\n",
    "    ExtraTreesClassifier(criterion='entropy', n_estimators=100),\n",
    "    ExtraTreesClassifier(criterion='entropy', n_estimators=500),\n",
    "    ExtraTreesClassifier(criterion='entropy', n_estimators=1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9 StackingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><center>StackingClassifier flow chart</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](StackingClassifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Base models are the best single classifier of each type (along with their pipes with scaling, encoding and feature selection) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9.1 Create Test StackingClassifier for selecting the best meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All base models are trained five times on different four folds combination and predicted results for rest fold, which will be a new meta-model features set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9.1.1 Initialize StackingClassifier and train base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data to 5 folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# it is a fast implementaion of StackClassifier, only use for selecting best meta model\n",
    "stack_clf_eval = FastStackClassifier(base_models=[\n",
    "                                                  clf_svc, \n",
    "                                                  clf_lr, \n",
    "                                                  clf_knn, \n",
    "                                                  clf_rbf,\n",
    "                                                  clf_rf, \n",
    "                                                  clf_xgb, \n",
    "                                                  clf_ada, \n",
    "                                                  clf_cat\n",
    "                                                 ],\n",
    "                                     kfold=kfold)\n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    stack_clf_eval.fit_base_models(X_train_eval, y_train_eval, X_val_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9.1.2 Select best meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 : LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.6848\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 1 : LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7091\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 2 : LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 3 : LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 4 : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 5 : LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 6 : LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7091\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 7 : LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 8 : LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 9 : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7182\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 10 : SVC(C=0.001, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.6848\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 11 : SVC(C=0.01, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.6939\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 12 : SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7212\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 13 : SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7242\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 14 : SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7242\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 15 : SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7242\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 16 : SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7242\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 17 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6727\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 18 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6727\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 19 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6939\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 20 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6788\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 21 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6485\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 22 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6697\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 23 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6879\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 24 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6879\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 25 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                     oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6303\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 26 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.7\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 27 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6818\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 28 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6818\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 29 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                     oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6576\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 30 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6939\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 31 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6758\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 32 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6909\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Best meta model: SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Best score: 0.7242\n"
     ]
    }
   ],
   "source": [
    "# function to selecting best meta model\n",
    "meta_model = stack_clf_eval.evaluate_meta_models(meta_models=[*meta_models_to_check], \n",
    "                                                 y_train=y_train_eval, \n",
    "                                                 y_test=y_val_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9.2 Create final StackClassifier to make test set prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9.2.1 Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data to 5 folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stack_clf = StackClassifier(base_models=[\n",
    "                                         clf_svc, \n",
    "                                         clf_lr, \n",
    "                                         clf_knn, \n",
    "                                         clf_rbf,\n",
    "                                         clf_rf, \n",
    "                                         clf_xgb, \n",
    "                                         clf_ada, \n",
    "                                         clf_cat\n",
    "                                        ],\n",
    "                            meta_model=meta_model, \n",
    "                            kfold=kfold)\n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    stack_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9.2.2 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# give model a name\n",
    "stack_clf_name = f'{stack_clf.__class__.__name__}'\n",
    "\n",
    "\n",
    "# add prediction metrics for stacking classifier to placeholder\n",
    "prediction_metrics.add_metrics(stack_clf, stack_clf_name, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.10 LargeStackingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><center>LargeStackingClassifier flow chart</center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LargeStackingClassifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Base models are best five classifiers of each type (along with their pipes with scaling, encoding and feature selection) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10.1 Create Test LargeStackingClassifier for selecting the best meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All base models are trained five times on different four folds combination and predicted results for rest fold, which will be a new meta-model features set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.10.1.1 Initialize LargeStackingClassifier and train base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data to 5 folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# it is a fast implementaion of StackClassifier, only use for selecting best meta model\n",
    "large_stack_clf_eval = FastStackClassifier(base_models=[\n",
    "                                                        *avg_clf_svc.base_estimators, \n",
    "                                                        *avg_clf_lr.base_estimators, \n",
    "                                                        *avg_clf_knn.base_estimators, \n",
    "                                                        *avg_clf_rbf.base_estimators,\n",
    "                                                        *avg_clf_rf.base_estimators, \n",
    "                                                        *avg_clf_xgb.base_estimators, \n",
    "                                                        *avg_clf_ada.base_estimators, \n",
    "                                                        *avg_clf_cat.base_estimators\n",
    "                                                       ],\n",
    "                                         kfold=kfold)\n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    large_stack_clf_eval.fit_base_models(X_train_eval, y_train_eval, X_val_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.10.1.2 Select best meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 : LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7212\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 1 : LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7333\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 2 : LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 3 : LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 4 : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 5 : LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7273\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 6 : LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7273\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 7 : LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 8 : LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 9 : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 10 : SVC(C=0.001, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.6879\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 11 : SVC(C=0.01, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7364\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 12 : SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7394\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 13 : SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7485\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 14 : SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7455\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 15 : SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7424\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 16 : SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Accuracy score: 0.7424\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 17 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.6879\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 18 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.7242\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 19 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.703\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 20 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.703\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 21 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.7212\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 22 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.7273\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 23 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.7273\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 24 : RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score: 0.7152\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 25 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                     oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6879\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 26 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.7212\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 27 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.7212\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 28 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.703\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 29 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "                     oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6636\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 30 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.6848\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 31 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.7061\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Model 32 : ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='entropy', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "Accuracy score: 0.7121\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Best meta model: SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "Best score: 0.7485\n"
     ]
    }
   ],
   "source": [
    "# function to selecting best meta model\n",
    "meta_model = large_stack_clf_eval.evaluate_meta_models(meta_models=[*meta_models_to_check], \n",
    "                                                       y_train=y_train_eval, \n",
    "                                                       y_test=y_val_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10.2 Create final LargeStackClassifier to make test set prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.10.2.1 Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data to 5 folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "large_stack_clf = StackClassifier(base_models=[\n",
    "                                               *avg_clf_svc.base_estimators, \n",
    "                                               *avg_clf_lr.base_estimators, \n",
    "                                               *avg_clf_knn.base_estimators, \n",
    "                                               *avg_clf_rbf.base_estimators,\n",
    "                                               *avg_clf_rf.base_estimators, \n",
    "                                               *avg_clf_xgb.base_estimators, \n",
    "                                               *avg_clf_ada.base_estimators, \n",
    "                                               *avg_clf_cat.base_estimators\n",
    "                                        ],\n",
    "                                    meta_model=meta_model, \n",
    "                                    kfold=kfold)\n",
    "\n",
    "# to safely run multiprocessing on Windows\n",
    "if __name__ == '__main__':\n",
    "    large_stack_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.10.2.2 Calculate metrics of prediction and add results to the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# give model a name\n",
    "large_stack_clf_name = f'Large{large_stack_clf.__class__.__name__}'\n",
    "\n",
    "\n",
    "# add prediction metrics for stacking classifier to placeholder\n",
    "prediction_metrics.add_metrics(large_stack_clf, large_stack_clf_name, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.11 Show all results in one table and save it for future purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AveragingAllModelsClassifier</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.740566</td>\n",
       "      <td>0.673684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LargeAveragingAllModelsClassifier</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.739218</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>StackClassifier</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.759659</td>\n",
       "      <td>0.710526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LargeStackClassifier</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.646707</td>\n",
       "      <td>0.749102</td>\n",
       "      <td>0.689474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  precision_score  recall_score  f1_score  \\\n",
       "0       AveragingAllModelsClassifier         0.630952      0.630952  0.630952   \n",
       "1  LargeAveragingAllModelsClassifier         0.633333      0.678571  0.655172   \n",
       "2                    StackClassifier         0.698630      0.607143  0.649682   \n",
       "3               LargeStackClassifier         0.650602      0.642857  0.646707   \n",
       "\n",
       "   roc_auc_score  accuracy_score  \n",
       "0       0.740566        0.673684  \n",
       "1       0.739218        0.684211  \n",
       "2       0.759659        0.710526  \n",
       "3       0.749102        0.689474  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get prediction metric result lists from placeholder\n",
    "precision_score, recall_score, f1_score, roc_auc_score, accuracy_score = prediction_metrics.get_metrics()\n",
    "\n",
    "# get model names list from placeholder\n",
    "models_name = prediction_metrics.get_names()\n",
    "\n",
    "# create dictionary of results \n",
    "results_dict = {'precision_score': precision_score, \n",
    "               'recall_score': recall_score, \n",
    "               'f1_score': f1_score,\n",
    "               'roc_auc_score' : roc_auc_score,\n",
    "               'accuracy_score' : accuracy_score}\n",
    "\n",
    "results_df = pd.DataFrame(data=results_dict)\n",
    "results_df.insert(loc=0, column='Model', value=models_name)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"./results/advance_ensembling_models_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
